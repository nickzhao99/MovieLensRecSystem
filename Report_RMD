---
title: "Movielens Recommendation System"
author: "Zhao, Jiahao"
date: "5/16/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

For this capstone project, the author created a movie recommendation system using the MovieLens dataset (10M). The dataset included six variables, with rating as the outcome variable of analysis. The goal of this project is to predict the score a specific movie will receive for each distinct user. The data is cleaned, explored, and modelled through a ridge regression to achieve optimal RMSE

## Methods and Analysis

In this section, the steps taken for data cleaning, data exploration, visualization, and the modeling approach is given and analyzed.

### Data, Libraries Loading and Download
The first step is to download the dataset and load it into the R environment. Libraries are also initialized for use later. Lastly, test dataset and train dataset are partitioned. edx will be the training dataset, while validation will be the testing dataset.

```{r Download and load dataset,message=FALSE,echo=FALSE, warning=FALSE}
################################
# Create edx set, validation set
################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(glmnet)) install.packages("glmnet", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")

# Load libraries
library(tidyverse)
library(caret)
library(data.table)
library(dplyr)
library(glmnet)

# MovieLens 10M dataset:
 # https://grouplens.org/datasets/movielens/10m/
 # http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
 download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
 colnames(movies) <- c("movieId", "title", "genres")
 movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                            title = as.character(title),
                                            genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
 edx <- movielens[-test_index,]
 temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
 edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```


#### Exploratory Data Analysis

The first thing is to verify the number of rows and columns in the training dataset. The code for the quizzes are given in the appendix section of this report.

```{r}
nrow(edx) #returns number of rows
ncol(edx) #returns number of columns
```

#### Variables 
userId: the ID for the specific user
movieId: the ID for the specific movie
rating: the outcome variable, the rating given by the specific user
timestamp: the time that the review was given
title: title of the movie
genres: genre of the movie


#### Data visualization
Next, the author creates a histogram for ratings, the outcome variable.
```{r}
#creates histogram of ratings
hist(edx$rating, main = "Histogram of ratings", xlab = "Rating", col ="Lightblue")
```
The histogram shows that 4 and 3 are the most given scores. Scores are skewed towards higher ratings.



#### Data manipulation
With 10M records, there must be a large number of distinct movies and users (those who provided the rating). Therefore, let's analyze the average number of ratings for movies, average user number of ratings. We will also calculate movie-specific average rating and user-specific average rating.
```{r}
edx <- edx %>% group_by(userId) %>% mutate(usernumratings = n())
# creates the column that displays the user's total number of ratings given

edx <- edx %>% group_by(movieId) %>% mutate(movienumratings = n())
# creates the column that displays the movie's total number of ratings

edx <- edx %>% group_by(movieId) %>% mutate(averagemovierating = (sum(rating))/movienumratings)
# creates the column that displays the movie's average ratings

edx <- edx %>% group_by(userId) %>% mutate(averageuserrating = (sum(rating))/usernumratings)
# creates the column that displays the user's average ratings given
```

Let's see if those newly created variables follows a normal distribution. If not, they will have to be normalized.
```{r}
hist(edx$movienumratings, main = "Histogram of # of Ratings for Movies",col="lightblue")
# creates histogram for number of ratings for movies
```

```{r}
hist(edx$usernumratings, main = "Histogram of # of Ratings for Users", col="lightblue")
# creates histogram for number of ratings for users
```

As you can see, these variables are not at all normally distributed. We will normalize these variables for to prevent OLS assumption violation and to fit the data better.


```{r}
mu <- mean(edx$usernumratings) # calcualtes mean
sdv <- sd(edx$usernumratings) # calculates standard deviation
edx <- edx %>% mutate(zusernumratings = (usernumratings-mu)/sdv)
# normalized usernumratings using the formula (xi-mu)/sd

mu <- mean(edx$movienumratings) # calcualtes mean
sdv <- sd(edx$movienumratings) # calculates standard deviation
edx <- edx %>% mutate(zmovienumratings = (movienumratings-mu)/sdv)
rm(mu,sdv)
# normalized movienumratings using the formula (xi-mu)/sd
```

#### Preparing the data for ridge regression
Multiple analysis for variables have been considered. The author performed the following procedures with the respective RMSE scores:
randomForest: 2.2
OLS with one variable: 1.1
OLS with genre as dummies and interactive terms: 0.871

The OLS linear regression model performs poorly in a situation where one has a large dataset with more variables than samples. Although we do have 10M samples and only a handful of variables: Each distinct user and movie can be considered as a dummy variable. Numerical scale does not work. Thus, the OLS will not perform well in this model, especially in terms of new test data and will likely cause overtraining due to its goal of minimizing least-squares error. The best approach is the ridge regression approach. Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity as described earlier. It is also shorter in computational time from state-of-the-art method known as the alternating least squares approach.

The glmnet package provides the functionality for ridge regression via glmnet(). 
Rather than accepting a formula and data frame, it requires a vector output vector and matrix of predictors.

First, we create the necessary vector and matrices.
```{r, warning=FALSE}
y <- edx$rating # creates outcome vector
x <- edx %>% select(zusernumratings,averageuserrating,zmovienumratings,averagemovierating) %>% data.matrix() # creates input matrix with 5 variables, the userId is automatically used.

```

The cv.fit function allows us to find the best penalty rate, lambda. 


```{r, cache=TRUE}
cv_fit <- cv.glmnet(x, y, alpha = 0, lambda = (c(0.465,0.475,0.047)))
# finds the best lambda penalty rate for x input, y output, and alpha of 0 to indicate a ridge regression using the glmnet package. The original method checks over the range of -2 and 5, but is now reduced to save significant amounts of time.
```



It is the lowest point in the curve. We can extract this values as:
```{r}
cv_fit$lambda.min # displays the best lambda penalty rate
```


With the lambda, we can now fit out model.
```{r}
model <- glmnet(x, y, alpha = 0, lambda = cv_fit$lambda.min)
#fits the model of x input matrix, y output vector, alpha of 0 to indicate ridge regression, and using the best lambda penalty rate found.
```


We also prepare the testing dataset for RMSE checking.
```{r, warning=FALSE}
validation <- validation %>% group_by(userId) %>% mutate(usernumratings = n())
# creates usernumrating column

validation <- validation %>% group_by(movieId) %>% mutate(movienumratings = n())
# creates movienumratings column
validation <- validation %>% group_by(movieId) %>% mutate(averagemovierating = (sum(rating))/movienumratings)
# creates average movie rating column
validation <- validation %>% group_by(userId) %>% mutate(averageuserrating = (sum(rating))/usernumratings)
# calcualtes and creates average rating given by the user

mu <- mean(validation$usernumratings)
sdv <- sd(validation$usernumratings)
validation <- validation %>% mutate(zusernumratings = (usernumratings-mu)/sdv)
# normalizes usernumratings

mu <- mean(validation$movienumratings)
sdv <- sd(validation$movienumratings)
validation <- validation %>% mutate(zmovienumratings = (movienumratings-mu)/sdv)
#normalizes movienumratings

test <- validation %>% select(zusernumratings,averageuserrating,zmovienumratings,averagemovierating) %>% data.matrix()
# creates matrix for input x to test model using the validation dataset.
```


## Results

After comparing between RandomForest, OLS, and ridge regression, the ridge regression provides the lowest RMSE on the testing dataset of 0.8443654. The ridge regression typically results in a higher RMSE on the training dataset, but is better on new data.


```{r}
predictions <- model %>% predict(test) %>% as.vector()
# predicts the test matrix using our model and outputs a vector.

data.frame(
     RMSE = RMSE(predictions, validation$rating),
     Rsquare = R2(predictions, validation$rating))

# Calculates the RMSE and R-squared value using predictions and rating in the validation dataset.

RMSE = RMSE(predictions, validation$rating)
# reports RMSE
RMSE
```

## Conclusion
The RMSE is 0.844. Below the 0.865 requirement. I learned a great deal about machine learning through this project. I also spent a lot of time creating many instances of different algorithms before finding the one that works. The best advice for myself is to know the strength and weaknesses of each algorithms before spending hours of time running them. 